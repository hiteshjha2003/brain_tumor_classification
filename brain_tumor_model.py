# -*- coding: utf-8 -*-
"""BrainTumorDetection_UsingTensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I6s56uHSVigIJwhKOCZi2pONO4zkhfbJ
"""

# ==================== Essential Imports ====================

# Import NumPy for numerical operations (arrays, matrices, etc.)
import numpy as np

# Import Pandas for data manipulation and analysis (e.g., DataFrames)
import pandas as pd

# Import matplotlib for plotting graphs
import matplotlib.pyplot as plt

# Import seaborn for statistical data visualization (built on matplotlib)
import seaborn as sns

# Import evaluation metrics from sklearn
from sklearn.metrics import (
    classification_report,  # To print precision, recall, f1-score, etc.
    confusion_matrix,       # To visualize predicted vs actual class performance
    roc_curve,              # For ROC curve (True Positive Rate vs False Positive Rate)
    auc                     # For calculating Area Under the ROC Curve
)

# For handling class imbalance by computing weights for each class
from sklearn.utils.class_weight import compute_class_weight

# Import TensorFlow library (deep learning framework)
import tensorflow as tf

# Import pre-trained CNN models for transfer learning
from tensorflow.keras.applications import EfficientNetB2, ResNet50, MobileNetV3Small

# Import layers used to build custom neural network models
from tensorflow.keras.layers import (
    Dense,                  # Fully connected layer
    GlobalAveragePooling2D, # Reduces feature maps to vector before Dense
    Dropout,                # Prevents overfitting by randomly dropping neurons
    BatchNormalization      # Normalizes activations, speeds up training
)

# Import Model class to define custom Keras models
from tensorflow.keras.models import Model

# Import the AdamW optimizer (Adam with Weight Decay Regularization)
from tensorflow.keras.optimizers import AdamW

# Import callbacks for better training control
from tensorflow.keras.callbacks import (
    EarlyStopping,         # Stops training early if no improvement
    ModelCheckpoint,       # Saves the best model automatically
    CSVLogger,             # Logs training details to a CSV file
    ReduceLROnPlateau      # Reduces learning rate when metric plateaus
)

# Import ImageDataGenerator for real-time image loading and augmentation
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Import OpenCV for image processing (reading, resizing images, etc.)
import cv2

# OS library to handle file paths and directory management
import os

# Suppress warning messages for cleaner output
import warnings
warnings.filterwarnings('ignore')


# ==================== TensorFlow and GPU Info ====================

# Print header
print("SYSTEM INFORMATION")
print("=" * 25)

# Print TensorFlow version installed
print(f"TensorFlow Version: {tf.__version__}")

# Check and print if GPU is available for TensorFlow
print(f"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}")

# If GPU is available, list all GPU devices
if len(tf.config.list_physical_devices('GPU')) > 0:
    print(f"GPU Devices: {tf.config.list_physical_devices('GPU')}")


# ==================== GPU Memory Growth Configuration ====================

# Get the list of physical GPU devices (experimental API)
gpus = tf.config.experimental.list_physical_devices('GPU')

# If GPUs are available
if gpus:
    try:
        # Loop through each GPU device
        for gpu in gpus:
            # Set memory growth to True
            # This allows TensorFlow to allocate GPU memory as needed (instead of grabbing all memory at once)
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        # If memory growth setting fails (e.g., after model is initialized), print the error
        print(e)

from google.colab import drive
drive.mount('/content/drive')

# ==================== 2. Model Architecture Comparison ====================
#  Summary of What This Code Does:
# Creates a comparison table of 3 popular CNN models using different criteria.

#  Selects EfficientNetB2 as the best model based on balance between:

# Accuracy

# Speed

# Memory usage

# Suitability for medical AI tasks

#  Visualizes the key metrics (Parameters, Accuracy, Speed) using bar charts side by side.

# Create a DataFrame containing performance metrics of different CNN models
model_comparison = pd.DataFrame({
    'Model': ['EfficientNetB2', 'ResNet50', 'MobileNetV3Small'],  # Names of the models
    'Parameters (M)': [9.1, 25.6, 2.9],                            # Number of trainable parameters (in millions)
    'Top-1 Accuracy (%)': [80.5, 76.0, 67.4],                      # Accuracy on a benchmark dataset (Top-1 prediction)
    'Inference Speed (ms)': [25, 45, 15],                          # How fast the model makes predictions (lower is better)
    'Memory Usage (MB)': [36, 102, 12],                            # How much memory the model consumes during inference
    'Medical AI Suitability': ['Excellent', 'Good', 'Fair']       # Subjective suitability for medical applications
})

# Print table heading for model comparison
print(" MODEL ARCHITECTURE COMPARISON")
print("=" * 35)

# Print the DataFrame as a neat table (no index shown)
print(model_comparison.to_string(index=False))

# Print final selected model based on the comparison
print("\nüéØ SELECTED MODEL: EfficientNetB2")
print("=" * 35)

# Justification text explaining why EfficientNetB2 was selected
print("Justification:")
print(" Optimal accuracy-speed tradeoff for medical applications")
print(" Proven performance on medical imaging tasks")
print(" Balanced parameter count (9.1M)")
print(" Meets <50ms inference requirement")
print(" Excellent transfer learning capabilities")

# Create a figure with 1 row and 3 bar chart subplots (for Params, Accuracy, Speed)
fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Width=15 inches, Height=5 inches

# Bar chart for number of parameters in each model
axes[0].bar(model_comparison['Model'], model_comparison['Parameters (M)'],
            color=['#2E8B57', '#4682B4', '#CD853F'])  # Custom bar colors for each model
axes[0].set_title('Parameters (Millions)', fontweight='bold')  # Set chart title
axes[0].set_ylabel('Parameters (M)')                            # Y-axis label
axes[0].tick_params(axis='x', rotation=45)                     # Rotate x-tick labels for readability


# Bar chart for accuracy of each model
axes[1].bar(model_comparison['Model'], model_comparison['Top-1 Accuracy (%)'],
            color=['#2E8B57', '#4682B4', '#CD853F'])
axes[1].set_title('Top-1 Accuracy (%)', fontweight='bold')  # Title of accuracy graph
axes[1].set_ylabel('Accuracy (%)')                          # Y-axis label
axes[1].tick_params(axis='x', rotation=45)                  # Rotate x-ticks


# Bar chart for inference speed of each model (lower is better)
axes[2].bar(model_comparison['Model'], model_comparison['Inference Speed (ms)'],
            color=['#2E8B57', '#4682B4', '#CD853F'])
axes[2].set_title('Inference Speed (ms)', fontweight='bold')  # Title for speed chart
axes[2].set_ylabel('Speed (ms)')                               # Y-axis label
axes[2].tick_params(axis='x', rotation=45)                     # Rotate x-tick labels

# Adjust spacing between subplots to prevent overlap
plt.tight_layout()

# Display all the bar charts
plt.show()

# ==================== 3. Dataset Loading & Exploration ====================

# Define possible dataset paths (useful when running on platforms like Google Colab or Kaggle)
dataset_paths = [
   "/content/drive/MyDrive/DATASETS/Brain_Tumor_Datasets/"
]

# Initialize variable to hold the correct dataset path
DATASET_PATH = None

# Check if any of the listed dataset paths actually exist
for path in dataset_paths:
    if os.path.exists(path):  # os.path.exists() checks if the folder/file exists on disk
        DATASET_PATH = path   # If found, assign that path to DATASET_PATH
        break                 # Stop checking once a valid path is found

# If no valid dataset path was found, print an error and expected folder structure
if DATASET_PATH is None:
    print(" Dataset not found. Please check dataset name and add it to your notebook.")
    print("Expected dataset structure:")
    print("- Training/glioma/")
    print("- Training/meningioma/")
    print("- Training/notumor/")
    print("- Training/pituitary/")
    print("- Testing/glioma/")
    print("- Testing/meningioma/")
    print("- Testing/notumor/")
    print("- Testing/pituitary/")
else:
    # Print the dataset path if found
    print(f" Dataset found at: {DATASET_PATH}")


# If dataset path is valid
if DATASET_PATH:
    # Define paths for Training and Testing directories
    TRAIN_DIR = f"{DATASET_PATH}/Training"
    TEST_DIR = f"{DATASET_PATH}/Testing"

    # Define image input size and training configuration
    IMG_SIZE = (224, 224)     # Target size to which all images will be resized
    BATCH_SIZE = 32           # Number of images per batch during training
    # EPOCHS = 20               # Total number of training iterations over the dataset

    EPOCHS = 1  #  Overwrites previous EPOCHS value (maybe for testing). This line seems accidental or used for debugging.


        # Define the 4 classes present in the dataset
    CLASS_NAMES = ['glioma', 'meningioma', 'notumor', 'pituitary']


    print(f"\n DATASET STRUCTURE")
    print("=" * 22)

    train_counts = {}  # Dictionary to store image counts for each class in training set

    if os.path.exists(TRAIN_DIR):  # Ensure training directory exists
        for class_name in CLASS_NAMES:
            class_path = os.path.join(TRAIN_DIR, class_name)  # Build path like: /Training/glioma/
            if os.path.exists(class_path):  # Check if class folder exists
                count = len(os.listdir(class_path))  # Count number of files (images) in the folder
                train_counts[class_name] = count     # Store count in dictionary
                print(f"Training {class_name}: {count} images")  # Print count


    test_counts = {}  # Dictionary to store image counts for each class in testing set

    if os.path.exists(TEST_DIR):  # Ensure testing directory exists
        print("\nTesting Set:")
        for class_name in CLASS_NAMES:
            class_path = os.path.join(TEST_DIR, class_name)  # Path like: /Testing/pituitary/
            if os.path.exists(class_path):
                count = len(os.listdir(class_path))  # Count number of test images
                test_counts[class_name] = count
                print(f"Testing {class_name}: {count} images")


    if train_counts:  # Proceed only if training data counts were collected
        # Create 1 row and 2 columns of subplots
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # ==== Training Set Distribution Chart ====
        ax1.bar(train_counts.keys(), train_counts.values(),
                color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])  # Custom colors
        ax1.set_title('Training Set Distribution', fontweight='bold')  # Chart title
        ax1.set_ylabel('Number of Images')  # Y-axis label
        ax1.tick_params(axis='x', rotation=45)  # Rotate x-axis labels

        # ==== Testing Set Distribution Chart ====
        if test_counts:  # Proceed only if testing data counts were collected
            ax2.bar(test_counts.keys(), test_counts.values(),
                    color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
            ax2.set_title('Testing Set Distribution', fontweight='bold')  # Chart title
            ax2.set_ylabel('Number of Images')  # Y-axis label
            ax2.tick_params(axis='x', rotation=45)  # Rotate x labels for readability

        # Adjust layout to avoid overlapping text and make it tight
        plt.tight_layout()

        # Show the plot
        plt.show()


        # Calculate the total number of training images
        total_train = sum(train_counts.values())

        print(f"\nCLASS DISTRIBUTION ANALYSIS")
        print("=" * 32)

        # Loop through each class to calculate its percentage in the dataset
        for class_name, count in train_counts.items():
            percentage = (count / total_train) * 100  # Formula: (count / total) * 100
            print(f"{class_name}: {count} ({percentage:.1f}%)")  # Print class distribution with percentage

# ==================== 4. MRI-Specific Data Preprocessing ====================

# Print header for the preprocessing step
print(" MRI-SPECIFIC PREPROCESSING")
print("=" * 35)

# Proceed only if the dataset was found earlier
if DATASET_PATH:

    # --------------------- TRAINING DATA GENERATOR WITH AUGMENTATION ---------------------

    # Create a training ImageDataGenerator with MRI-specific data augmentation techniques
    train_datagen = ImageDataGenerator(
        preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,
        # Preprocess input based on EfficientNet standard (normalize pixel values etc.)

        rotation_range=15,            # Randomly rotate images up to 15 degrees (MRI machines might capture slightly rotated images)
        zoom_range=0.2,               # Zoom in/out by 20% to simulate magnification differences
        width_shift_range=0.1,        # Randomly shift image horizontally by 10% of width
        height_shift_range=0.1,       # Randomly shift image vertically by 10% of height
        horizontal_flip=True,         # Flip images horizontally; brains are symmetric, so this is valid
        brightness_range=[0.8, 1.2],  # Randomly adjust brightness/contrast (important in MRI imaging)
        fill_mode='nearest',          # Filling strategy for newly created pixels (e.g., during rotation/shift)
        validation_split=0.2          # Reserve 20% of training data for validation (no need for separate folder)
    )

    # --------------------- TEST DATA GENERATOR (NO AUGMENTATION) ---------------------

    test_datagen = ImageDataGenerator(
        preprocessing_function=tf.keras.applications.efficientnet.preprocess_input
        # No augmentations for test data ‚Äî it should represent real unseen examples
    )

    # --------------------- CREATING DATA GENERATOR OBJECTS ---------------------

    try:
        # TRAINING generator (from Training directory, training subset of 80%)
        train_generator = train_datagen.flow_from_directory(
            TRAIN_DIR,
            target_size=IMG_SIZE,        # Resize all images to (224, 224)
            batch_size=BATCH_SIZE,       # Number of images per batch
            class_mode='categorical',    # One-hot encoded labels for multiclass classification
            subset='training',           # Use 80% of data for training
            shuffle=True                 # Shuffle images to avoid learning sequence
        )

        # VALIDATION generator (from Training directory, validation subset of 20%)
        validation_generator = train_datagen.flow_from_directory(
            TRAIN_DIR,
            target_size=IMG_SIZE,
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            subset='validation',         # Use 20% of data for validation
            shuffle=False                # Do not shuffle for validation, helps in reproducibility
        )

        # TEST generator (from Testing directory, no augmentations)
        test_generator = test_datagen.flow_from_directory(
            TEST_DIR,
            target_size=IMG_SIZE,
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            shuffle=False                # Do not shuffle to match predictions with filenames
        )

        # Confirmation messages
        print(" Data generators created successfully")
        print(f"Training samples: {train_generator.samples}")
        print(f"Validation samples: {validation_generator.samples}")
        print(f"Test samples: {test_generator.samples}")
        print(f"Number of classes: {train_generator.num_classes}")

        # Print class label-to-index mapping, e.g., {'glioma': 0, 'meningioma': 1, ...}
        print(f"\nClass indices: {train_generator.class_indices}")

    except Exception as e:
        # Handle errors (e.g., missing directories or malformed data)
        print(f" Error creating data generators: {e}")
        train_generator = None
        validation_generator = None
        test_generator = None


# If the dataset and training generator were created successfully
if DATASET_PATH and train_generator:
    print("\n SAMPLE IMAGES WITH AUGMENTATION")
    print("=" * 37)

    # Fetch a single batch of training images and labels
    sample_batch = next(train_generator)    # Returns (images, labels)
    sample_images = sample_batch[0]         # Extract images from the batch
    sample_labels = sample_batch[1]         # Extract labels (one-hot encoded)

    # Create a 2x4 grid (8 subplots) for displaying sample images
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))  # 2 rows, 4 columns
    axes = axes.ravel()  # Flatten the axes array for easier indexing

    for i in range(8):
        if i < len(sample_images):
            img = sample_images[i]

            # ============================
            # Denormalize image for display
            # EfficientNet preprocess_input scales pixel values between -1 and 1.
            # We scale them back to [0, 1] for display using (x - min) / (max - min)
            img = (img - img.min()) / (img.max() - img.min())

            # Plot the image
            axes[i].imshow(img)

            # Convert one-hot encoded label back to class name
            class_idx = np.argmax(sample_labels[i])  # Get index of the class
            class_name = list(train_generator.class_indices.keys())[class_idx]  # Map index to class name

            # Set image title to class name
            axes[i].set_title(f'Class: {class_name}', fontweight='bold')
            axes[i].axis('off')  # Hide axis ticks

    # Set overall title for all images
    plt.suptitle('Sample MRI Images with Augmentation', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()  # Display all 8 images

# ==================== 6. Custom Focal Loss Implementation ====================

# Define a custom loss class that inherits from tf.keras.losses.Loss
# Key Concepts Recap
# Œ± (alpha): A scaling factor to emphasize the loss for minority classes.

# Œ≥ (gamma): A focusing parameter that reduces the influence of easy-to-classify examples.

# This makes hard examples (like borderline tumor images) more important in the loss, leading to better performance in class-imbalanced scenarios

class FocalLoss(tf.keras.losses.Loss):
    """
    Focal Loss for addressing class imbalance in medical datasets

    Formula:
        Focal Loss = -Œ±(1 - p_t)^Œ≥ * log(p_t)

    Where:
    - p_t is the predicted probability for the true class.
    - Œ± is the balancing factor to give more weight to underrepresented classes.
    - Œ≥ (gamma) reduces the loss contribution from easy examples and focuses on hard examples.
    """

    # Constructor: initialize alpha, gamma, and any additional kwargs
    def __init__(self, alpha=1.0, gamma=2.0, **kwargs):
        super().__init__(**kwargs)
        self.alpha = alpha  # Weighting factor for class imbalance
        self.gamma = gamma  # Focusing parameter to reduce loss for easy examples

    # This function defines the actual focal loss calculation
    def call(self, y_true, y_pred):
        # -----------------------------------------------
        # Step 1: Clip predictions to avoid log(0)
        # -----------------------------------------------
        # y_pred might contain values like 0 or 1 which cause log(0) = -inf
        y_pred = tf.clip_by_value(y_pred, 1e-8, 1.0 - 1e-8)

        # -----------------------------------------------
        # Step 2: Compute standard cross-entropy
        # -----------------------------------------------
        # Formula: CE = -y_true * log(y_pred)
        # Only the correct class contributes to the loss
        ce_loss = -y_true * tf.math.log(y_pred)  # shape: (batch_size, num_classes)

        # -----------------------------------------------
        # Step 3: Compute focal modulation factor (1 - p_t)^Œ≥
        # -----------------------------------------------
        # p_t is the model's estimated probability for the correct class
        # For each element, if y_true is 1: p_t = y_pred, else: p_t = 1 - y_pred
        # This helps identify hard examples (lower p_t) and apply more weight
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)

        # Apply the focal weighting factor
        focal_weight = self.alpha * tf.pow(1 - p_t, self.gamma)  # shape: (batch_size, num_classes)

        # -----------------------------------------------
        # Step 4: Final focal loss = focal_weight * CE
        # -----------------------------------------------
        focal_loss = focal_weight * ce_loss  # shape: (batch_size, num_classes)

        # -----------------------------------------------
        # Step 5: Reduce the loss across classes and samples
        # -----------------------------------------------
        # First sum over classes, then take mean across the batch
        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=1))

    # Save configuration to support model saving/loading
    def get_config(self):
        config = super().get_config()
        config.update({
            'alpha': self.alpha,
            'gamma': self.gamma
        })
        return config


print("üéØ FOCAL LOSS IMPLEMENTATION")
print("=" * 32)
print("‚úÖ Custom Focal Loss defined")
print("Parameters:")
print("- Alpha (Œ±): 1.0 (weighting factor)")
print("- Gamma (Œ≥): 2.0 (focusing parameter)")
print("\nPurpose:")
print("- Address class imbalance in medical datasets")
print("- Focus learning on hard examples")
print("- Reduce impact of easy negatives")

# ==================== 7. Model Architecture Design ====================

def create_brain_tumor_model():
    """
    Creates a deep learning model based on EfficientNetB2 for brain tumor classification.

    Architecture Summary:
    - Pre-trained EfficientNetB2 as feature extractor (no top layer).
    - Global Average Pooling to reduce spatial dimensions.
    - Dense layers for classification.
    - Dropout and BatchNormalization for regularization and training stability.
    - Final softmax layer for multiclass prediction (4 tumor classes).
    """

    # Load EfficientNetB2 without top classification layer
    base_model = EfficientNetB2(
        weights='imagenet',        # Use pre-trained weights from ImageNet
        include_top=False,         # Exclude final classification layer
        input_shape=(224, 224, 3)  # Input image size (height, width, channels)
    )

    # Freeze the base model (all layers) to prevent its weights from being updated initially
    base_model.trainable = False

    # ------------------------- CUSTOM CLASSIFICATION HEAD -------------------------

    # Get the output of the base model
    x = base_model.output

    # Global Average Pooling reduces each feature map to a single value (less prone to overfitting than Flatten)
    x = GlobalAveragePooling2D(name='global_avg_pool')(x)

    # Batch Normalization stabilizes learning and accelerates convergence
    x = BatchNormalization(name='bn_1')(x)

    # Dropout randomly deactivates neurons during training to prevent overfitting
    x = Dropout(0.3, name='dropout_1')(x)

    # First Dense layer: 512 units + ReLU activation
    x = Dense(512, activation='relu', name='dense_1')(x)

    # Add another batch normalization layer to maintain healthy activations
    x = BatchNormalization(name='bn_2')(x)

    # More dropout for further regularization
    x = Dropout(0.3, name='dropout_2')(x)

    # Second Dense layer: 256 units + ReLU
    x = Dense(256, activation='relu', name='dense_2')(x)

    # Final dropout before the output layer
    x = Dropout(0.2, name='dropout_3')(x)

    # Output layer: 4 units for 4 tumor classes with softmax to output probabilities
    predictions = Dense(4, activation='softmax', name='predictions')(x)

    # Assemble the final model by linking base model input to custom head output
    model = Model(inputs=base_model.input, outputs=predictions, name='BrainTumorDetector')

    return model


# Print model architecture section header
print("üèóÔ∏è MODEL ARCHITECTURE")
print("=" * 25)

# Call the function to create the model instance
model = create_brain_tumor_model()

# Print the model's name
print(f"Model name: {model.name}")

# Print total number of parameters (trainable + non-trainable)
print(f"Total parameters: {model.count_params():,}")

# Calculate and print trainable parameters using Keras backend utilities
print(f"Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}")

# Calculate and print non-trainable parameters (mostly from frozen EfficientNetB2 base)
print(f"Non-trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights]):,}")

# Print a detailed summary of the model: layers, shapes, parameters
model.summary()


# Visualize and save the model architecture to a PNG file
tf.keras.utils.plot_model(
    model,
    to_file='model_architecture.png',     # File to save
    show_shapes=True,                     # Show layer input/output shapes
    show_layer_names=True,                # Show layer names
    rankdir='TB',                         # Top-to-bottom layout
    expand_nested=False,                  # Don't expand nested models
    dpi=96                                # Image resolution
)


print("\nüéØ Architecture Highlights:")
print("‚úÖ EfficientNetB2 backbone (9.1M parameters)")  # Efficient but powerful
print("‚úÖ Global Average Pooling for spatial reduction")  # Compact representation
print("‚úÖ Batch normalization for training stability")     # Helps gradient flow
print("‚úÖ Dropout layers for regularization")              # Reduces overfitting
print("‚úÖ Custom classification head")                     # Specialized for 4-class tumor classification
print("‚úÖ Softmax output for 4-class prediction")          # Multiclass prediction

# ==================== 8. Model Compilation & Callbacks ====================

# Section title for clarity
print(" MODEL COMPILATION")
print("=" * 22)

# ---------------------- MODEL COMPILATION ----------------------

# Compile the Keras model using the custom Focal Loss and AdamW optimizer
model.compile(
    optimizer=AdamW(                    # Advanced optimizer with decoupled weight decay
        learning_rate=1e-4,            # Initial learning rate
        weight_decay=1e-4              # L2 regularization to prevent overfitting
    ),
    loss=FocalLoss(gamma=2.0),         # Custom focal loss to handle class imbalance
    metrics=[
        'accuracy',                    # Basic accuracy metric
        tf.keras.metrics.Precision(name='precision'),  # How many predicted positives are actually positive
        tf.keras.metrics.Recall(name='recall'),        # How many actual positives are correctly predicted
        tf.keras.metrics.AUC(name='auc')               # Area Under the ROC Curve for performance
    ]
)

# Confirm successful compilation
print("‚úÖ Model compiled successfully")
print("Configuration:")
print("- Optimizer: AdamW (lr=1e-4, weight_decay=1e-4)")
print("- Loss: Focal Loss (gamma=2.0)")
print("- Metrics: Accuracy, Precision, Recall, AUC")

# Header for callback configuration
print("\nüìã CALLBACKS SETUP")
print("=" * 20)

# Define list of callbacks to be used during training
callbacks = [

    # ---------------------- EarlyStopping ----------------------
    # Stop training if validation recall doesn't improve for 5 consecutive epochs
    EarlyStopping(
        monitor='val_recall',             # Metric to monitor
        patience=5,                       # Number of epochs to wait before stopping
        restore_best_weights=True,       # Load the best weights (not the final weights)
        verbose=1,                        # Print when stopped
        mode='max'                        # Higher recall is better
    ),

    # ---------------------- ModelCheckpoint ----------------------
    # Save the best model to file when validation recall improves
    ModelCheckpoint(
        'best_brain_tumor_model.h5',      # Filepath to save the best model
        monitor='val_recall',             # Save model based on best validation recall
        save_best_only=True,              # Save only the best (not all epochs)
        verbose=1,                        # Log when model is saved
        mode='max'                        # Higher recall = better
    ),

    # ---------------------- CSVLogger ----------------------
    # Logs all training metrics to CSV file for further analysis
    CSVLogger(
        'training_log.csv',               # File to save training logs
        append=True                       # Continue logging if file already exists
    ),

    # ---------------------- ReduceLROnPlateau ----------------------
    # Reduces learning rate by factor of 0.5 if val_loss doesn't improve
    ReduceLROnPlateau(
        monitor='val_loss',               # Watch validation loss
        factor=0.5,                       # Reduce LR by half
        patience=3,                       # Wait 3 epochs before reducing
        min_lr=1e-7,                      # Do not reduce below this value
        verbose=1                         # Print when LR is reduced
    )
]

# Confirm callback setup
print("‚úÖ Callbacks configured:")
print("- EarlyStopping: monitor='val_recall', patience=5")
print("- ModelCheckpoint: save best model based on val_recall")
print("- CSVLogger: log training metrics to CSV")
print("- ReduceLROnPlateau: adaptive learning rate")


print(f"\n‚öôÔ∏è TRAINING CONFIGURATION")
print("=" * 27)
print(f"Image size: {IMG_SIZE}")               # e.g., (224, 224)
print(f"Batch size: {BATCH_SIZE}")             # e.g., 32
print(f"Epochs: {EPOCHS}")                     # e.g., 20
print(f"Initial learning rate: 1e-4")          # Matches what was set in AdamW
print(f"Optimizer: AdamW with weight decay")   # Advanced optimizer
print(f"Loss function: Focal Loss (Œ≥=2.0)")     # Best for class imbalance
print(f"Target metric: Recall > 98%")           # Recall is crucial in medical diagnosis (minimize false negatives)

# ==================== 9. MODEL TRAINING ====================
# Print section title
print(" MODEL TRAINING")
print("=" * 18)

# Check if dataset path exists before training
if DATASET_PATH:
    print("Re-creating data generators and calculating class weights...")

    try:
        # ------------------- Data Generators -------------------
        # Re-initialize ImageDataGenerator with MRI-specific augmentations
        train_datagen = ImageDataGenerator(
            preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,
            rotation_range=15,               # Slight rotation to simulate real MRI angle variance
            zoom_range=0.2,                  # Zoom in/out
            width_shift_range=0.1,           # Shift images horizontally
            height_shift_range=0.1,          # Shift images vertically
            horizontal_flip=True,            # Flip image horizontally to augment symmetry
            brightness_range=[0.8, 1.2],     # Simulate different brightness levels
            fill_mode='nearest',             # Fill any gaps created during transforms
            validation_split=0.2             # Split 20% of training data as validation
        )

        test_datagen = ImageDataGenerator(
            preprocessing_function=tf.keras.applications.efficientnet.preprocess_input
        )

        # Generate training data from directory
        train_generator = train_datagen.flow_from_directory(
            TRAIN_DIR,
            target_size=IMG_SIZE,
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            subset='training',
            shuffle=True
        )

        # Generate validation data from directory
        validation_generator = train_datagen.flow_from_directory(
            TRAIN_DIR,
            target_size=IMG_SIZE,
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            subset='validation',
            shuffle=False
        )

        # Check if test_generator was created earlier; if not, re-create it
        if 'test_generator' not in locals() or test_generator is None:
            test_generator = test_datagen.flow_from_directory(
                TEST_DIR,
                target_size=IMG_SIZE,
                batch_size=BATCH_SIZE,
                class_mode='categorical',
                shuffle=False
            )

        # ‚úÖ Confirmation print statements
        print("‚úÖ Data generators created successfully")
        print(f"Training samples: {train_generator.samples}")
        print(f"Validation samples: {validation_generator.samples}")
        print(f"Test samples: {test_generator.samples}")
        print(f"Number of classes: {train_generator.num_classes}")

        # ------------------- Class Weight Calculation -------------------
        # Computes class weights to handle class imbalance in training data
        class_weights = compute_class_weight(
            class_weight='balanced',
            classes=np.unique(train_generator.classes),  # Unique class labels
            y=train_generator.classes                    # Actual class distribution
        )

        # Convert to dictionary (Keras expects this format)
        class_weight_dict = dict(enumerate(class_weights))

        print("\n‚úÖ Class weights calculated:")
        print(class_weight_dict)

    except Exception as e:
        print(f"‚ö†Ô∏è Error re-creating data generators or calculating class weights: {e}")
        train_generator = None
        validation_generator = None
        test_generator = None
        class_weight_dict = None



# Proceed to training only if everything is set up correctly
if train_generator and validation_generator and class_weight_dict is not None:
    print("Starting training...")
    print(f"Training samples: {train_generator.samples}")
    print(f"Validation samples: {validation_generator.samples}")
    print(f"Steps per epoch: {train_generator.samples // BATCH_SIZE}")
    print(f"Validation steps: {validation_generator.samples // BATCH_SIZE}")

    try:
        # ------------------- Model Training -------------------
        history = model.fit(
            train_generator,                         # Training data generator
            steps_per_epoch=train_generator.samples // BATCH_SIZE,  # Steps per epoch
            epochs=EPOCHS,                            # Total number of training epochs
            validation_data=validation_generator,     # Validation generator
            validation_steps=validation_generator.samples // BATCH_SIZE,
            class_weight=class_weight_dict,           # Apply class weighting for imbalance
            callbacks=callbacks,                      # Use callbacks (EarlyStopping, LR scheduler, etc.)
            verbose=1                                 # Verbose = show progress bar
        )

        print("‚úÖ Training completed successfully!")

        # ------------------- Save History to CSV -------------------
        hist_df = pd.DataFrame(history.history)         # Convert training history to DataFrame
        hist_df.to_csv('training_history.csv', index=False)  # Save history to file
        print("‚úÖ Training history saved to training_history.csv")

    except Exception as e:
        print(f"‚ö†Ô∏è Training error: {e}")
        history = None  # Set history to None for safety

else:
    print("‚ö†Ô∏è Data generators, class weights, or dataset path not available. Please check dataset path and previous steps.")
    history = None

# Show training summary only if training completed
if history:
    print(f"\nüìä TRAINING SUMMARY")
    print("=" * 20)

    # Number of completed epochs
    final_epoch = len(history.history['loss']) - 1
    print(f"Training completed in {final_epoch + 1} epochs")

    # Extract final values from training history
    final_metrics = {
        'Training Accuracy': history.history['accuracy'][-1],
        'Validation Accuracy': history.history['val_accuracy'][-1],
        'Training Recall': history.history['recall'][-1],
        'Validation Recall': history.history['val_recall'][-1],
        'Training Precision': history.history['precision'][-1],
        'Validation Precision': history.history['val_precision'][-1]
    }

    # Print final metrics
    for metric, value in final_metrics.items():
        print(f"{metric}: {value:.4f}")

    # ------------------- Check Target Recall -------------------
    target_recall = 0.98
    if final_metrics['Validation Recall'] >= target_recall:
        print(f"\nüéØ ‚úÖ Target recall >{target_recall*100}% ACHIEVED!")
    else:
        print(f"\nüéØ ‚ö†Ô∏è Target recall >{target_recall*100}% not achieved")
        print("Consider fine-tuning hyperparameters or training longer")

#  Check if model training history exists before visualizing
if history:
    print(" TRAINING VISUALIZATION")
    print("=" * 27)

    # Create a 2x2 grid of subplots for visualizing loss, accuracy, precision, recall
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # 2 rows, 2 columns of plots

    # Set the overall title for the full plot
    fig.suptitle('Training History - Brain Tumor Detection Model', fontsize=16, fontweight='bold')


       # Plot 1: Loss
    axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2, color='#e74c3c')
    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='#3498db')
    axes[0, 0].set_title('Model Loss', fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()       # Add legend to identify curves
    axes[0, 0].grid(True, alpha=0.3)  # Add subtle grid for better readability


       # Plot 2: Accuracy
    axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='#e74c3c')
    axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='#3498db')
    axes[0, 1].set_title('Model Accuracy', fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)


        # Plot 3: Precision
    axes[1, 0].plot(history.history['precision'], label='Training Precision', linewidth=2, color='#e74c3c')
    axes[1, 0].plot(history.history['val_precision'], label='Validation Precision', linewidth=2, color='#3498db')
    axes[1, 0].set_title('Model Precision', fontweight='bold')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Precision')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)


        # Plot 4: Recall (Most important for medical applications)
    axes[1, 1].plot(history.history['recall'], label='Training Recall', linewidth=2, color='#e74c3c')
    axes[1, 1].plot(history.history['val_recall'], label='Validation Recall', linewidth=2, color='#3498db')

    # Draw a horizontal line indicating the target recall (e.g., 98%)
    axes[1, 1].axhline(y=0.98, color='#2ecc71', linestyle='--', linewidth=2, label='Target Recall (98%)')

    axes[1, 1].set_title('Model Recall (Critical for Medical AI)', fontweight='bold')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Recall')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)


    plt.tight_layout()
    plt.show()

       # Summary of training results
    print(f"\n TRAINING STATISTICS")
    print("=" * 25)

    # Epoch with the highest validation recall
    best_epoch = np.argmax(history.history['val_recall'])  # Returns index of max recall
    print(f"Best epoch: {best_epoch + 1}")  # +1 because epochs start at 1 for humans

    # Print best recall and accuracy values
    print(f"Best validation recall: {history.history['val_recall'][best_epoch]:.4f}")
    print(f"Best validation accuracy: {history.history['val_accuracy'][best_epoch]:.4f}")


    print(f"\n CONVERGENCE ANALYSIS")
    print("=" * 25)

    # Get last recorded training and validation accuracy
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]

    # Calculate the gap (difference)
    acc_gap = final_train_acc - final_val_acc

print(" MODEL EVALUATION")
print("=" * 18)

# Step 1: Recreate the model architecture
try:
    best_model = create_brain_tumor_model()
    best_model = best_model.to(device)
    print("‚úÖ Model architecture recreated successfully.")

    # Step 2: Load the best saved model weights (.pth file)
    best_model.load_state_dict(torch.load('best_brain_tumor_model.pth', map_location=device))
    best_model.eval()  # Set model to evaluation mode
    print("‚úÖ Best model weights loaded successfully!")

except Exception as e:
    print(f"‚ö†Ô∏è Error recreating model or loading weights: {e}")
    best_model = None

# Step 3: Evaluate on test dataset if everything is ready
if best_model is not None and 'test_loader' in locals():

    print("Evaluating model on the test dataset...")

    all_preds = []
    all_targets = []
    running_loss = 0.0
    criterion = FocalLoss(gamma=2.0)  # Use same loss as training

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = best_model(inputs)

            loss = criterion(outputs, targets)
            running_loss += loss.item() * inputs.size(0)

            preds = outputs.argmax(dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_targets.extend(targets.cpu().numpy())

    # Calculate metrics
    avg_loss = running_loss / len(test_loader.dataset)

    from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score

    accuracy = accuracy_score(all_targets, all_preds)
    precision = precision_score(all_targets, all_preds, average='macro', zero_division=0)
    recall = recall_score(all_targets, all_preds, average='macro', zero_division=0)

    # AUC calculation for multi-class (one-vs-rest)
    try:
        from sklearn.preprocessing import label_binarize
        n_classes = len(CLASS_NAMES)
        targets_binarized = label_binarize(all_targets, classes=range(n_classes))
        outputs_np = []

        # We need raw probabilities for AUC, so re-run with softmax probs
        with torch.no_grad():
            probs_list = []
            for inputs, _ in test_loader:
                inputs = inputs.to(device)
                outputs = best_model(inputs)
                probs_list.append(outputs.cpu().numpy())
            outputs_np = np.vstack(probs_list)

        auc = roc_auc_score(targets_binarized, outputs_np, average='macro', multi_class='ovr')
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Unable to calculate AUC: {e}")
        auc = float('nan')

    print("\n‚úÖ Test set evaluation results:")
    print(f"  Loss: {avg_loss:.4f}")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  AUC: {auc:.4f}")

    # Check target recall threshold
    target_recall = 0.98
    if recall >= target_recall:
        print(f"\nüéØ ‚úÖ Target recall >{target_recall*100}% ACHIEVED on Test Set!")
    else:
        print(f"\nüéØ ‚ö†Ô∏è Target recall >{target_recall*100}% not achieved on Test Set.")

else:
    print("‚ö†Ô∏è Best model or test_loader not available for evaluation. Please ensure model training was successful and test_loader is defined.")

